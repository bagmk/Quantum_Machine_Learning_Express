{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data3c.txt'\n",
    "data0Label = r'../../../dataset/data3clabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('Î¸'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.rx(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  28.216541290283203\n",
      "__Loss is  28.18442153930664\n",
      "__Loss is  28.1491756439209\n",
      "__Loss is  28.110624313354492\n",
      "__Loss is  28.068599700927734\n",
      "__Loss is  28.022951126098633\n",
      "__Loss is  27.973543167114258\n",
      "__Loss is  27.920272827148438\n",
      "__Loss is  27.863109588623047\n",
      "__Loss is  27.802043914794922\n",
      "Learning Rate is  0.04\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  28.07938003540039\n",
      "__Loss is  27.890155792236328\n",
      "__Loss is  27.6427001953125\n",
      "__Loss is  27.340974807739258\n",
      "__Loss is  27.00330352783203\n",
      "__Loss is  26.659799575805664\n",
      "__Loss is  26.3419246673584\n",
      "__Loss is  26.070987701416016\n",
      "__Loss is  25.853771209716797\n",
      "Learning Rate is  0.09\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  27.878459930419922\n",
      "__Loss is  27.28525161743164\n",
      "__Loss is  26.525386810302734\n",
      "__Loss is  25.893037796020508\n",
      "__Loss is  25.529739379882812\n",
      "__Loss is  25.347543716430664\n",
      "__Loss is  25.25246238708496\n",
      "__Loss is  25.198205947875977\n",
      "__Loss is  25.16435432434082\n",
      "Learning Rate is  0.16\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  27.55464744567871\n",
      "__Loss is  26.29161262512207\n",
      "__Loss is  25.44475746154785\n",
      "__Loss is  25.22350311279297\n",
      "__Loss is  25.153104782104492\n",
      "__Loss is  25.120458602905273\n",
      "__Loss is  25.101268768310547\n",
      "__Loss is  25.08834457397461\n",
      "__Loss is  25.078956604003906\n",
      "Learning Rate is  0.25\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  27.095983505249023\n",
      "__Loss is  25.40215301513672\n",
      "__Loss is  25.147682189941406\n",
      "__Loss is  25.10414695739746\n",
      "__Loss is  25.083961486816406\n",
      "__Loss is  25.071683883666992\n",
      "__Loss is  25.063596725463867\n",
      "__Loss is  25.058156967163086\n",
      "__Loss is  25.054494857788086\n",
      "Learning Rate is  0.36\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  26.530818939208984\n",
      "__Loss is  25.09706687927246\n",
      "__Loss is  25.075345993041992\n",
      "__Loss is  25.062955856323242\n",
      "__Loss is  25.055850982666016\n",
      "__Loss is  25.051870346069336\n",
      "__Loss is  25.049684524536133\n",
      "__Loss is  25.04848861694336\n",
      "__Loss is  25.047836303710938\n",
      "Learning Rate is  0.49\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.944564819335938\n",
      "__Loss is  25.071887969970703\n",
      "__Loss is  25.055055618286133\n",
      "__Loss is  25.05034637451172\n",
      "__Loss is  25.048519134521484\n",
      "__Loss is  25.0477352142334\n",
      "__Loss is  25.04738998413086\n",
      "__Loss is  25.047218322753906\n",
      "__Loss is  25.04714584350586\n",
      "Learning Rate is  0.64\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.461271286010742\n",
      "__Loss is  25.071382522583008\n",
      "__Loss is  25.053003311157227\n",
      "__Loss is  25.04901695251465\n",
      "__Loss is  25.0477294921875\n",
      "__Loss is  25.04730987548828\n",
      "__Loss is  25.047155380249023\n",
      "__Loss is  25.047117233276367\n",
      "__Loss is  25.047090530395508\n",
      "Learning Rate is  0.81\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.17490577697754\n",
      "__Loss is  25.058969497680664\n",
      "__Loss is  25.048765182495117\n",
      "__Loss is  25.04739761352539\n",
      "__Loss is  25.047143936157227\n",
      "__Loss is  25.04709815979004\n",
      "__Loss is  25.047086715698242\n",
      "__Loss is  25.047090530395508\n",
      "__Loss is  25.047080993652344\n",
      "Learning Rate is  1.0\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.074386596679688\n",
      "__Loss is  25.04947853088379\n",
      "__Loss is  25.04725456237793\n",
      "__Loss is  25.04709815979004\n",
      "__Loss is  25.047086715698242\n",
      "__Loss is  25.047086715698242\n",
      "__Loss is  25.047082901000977\n",
      "__Loss is  25.047080993652344\n",
      "__Loss is  25.047080993652344\n",
      "Learning Rate is  1.21\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.079408645629883\n",
      "__Loss is  25.048921585083008\n",
      "__Loss is  25.047155380249023\n",
      "__Loss is  25.047082901000977\n",
      "__Loss is  25.047082901000977\n",
      "__Loss is  25.047077178955078\n",
      "__Loss is  25.04707908630371\n",
      "__Loss is  25.04707908630371\n",
      "__Loss is  25.04707908630371\n",
      "Learning Rate is  1.44\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.238317489624023\n",
      "__Loss is  25.19536590576172\n",
      "__Loss is  25.05835723876953\n",
      "__Loss is  25.047117233276367\n",
      "__Loss is  25.04707908630371\n",
      "__Loss is  25.047077178955078\n",
      "__Loss is  25.04707908630371\n",
      "__Loss is  25.04707908630371\n",
      "__Loss is  25.04707908630371\n",
      "Learning Rate is  1.69\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.82965087890625\n",
      "__Loss is  25.222431182861328\n",
      "__Loss is  25.332815170288086\n",
      "__Loss is  25.81235122680664\n",
      "__Loss is  25.094270706176758\n",
      "__Loss is  25.072301864624023\n",
      "__Loss is  25.0518798828125\n",
      "__Loss is  25.047744750976562\n",
      "__Loss is  25.047134399414062\n",
      "Learning Rate is  1.96\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  26.940650939941406\n",
      "__Loss is  25.365795135498047\n",
      "__Loss is  25.479671478271484\n",
      "__Loss is  27.79250717163086\n",
      "__Loss is  25.32164192199707\n",
      "__Loss is  26.83085823059082\n",
      "__Loss is  26.495737075805664\n",
      "__Loss is  27.421627044677734\n",
      "__Loss is  25.188018798828125\n",
      "Learning Rate is  2.25\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  27.826433181762695\n",
      "__Loss is  26.70897674560547\n",
      "__Loss is  25.411970138549805\n",
      "__Loss is  27.076589584350586\n",
      "__Loss is  26.715009689331055\n",
      "__Loss is  25.394886016845703\n",
      "__Loss is  26.78558349609375\n",
      "__Loss is  25.386863708496094\n",
      "__Loss is  26.084651947021484\n",
      "Learning Rate is  2.56\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  27.415109634399414\n",
      "__Loss is  25.120466232299805\n",
      "__Loss is  25.381772994995117\n",
      "__Loss is  28.39521598815918\n",
      "__Loss is  26.380126953125\n",
      "__Loss is  25.06671905517578\n",
      "__Loss is  25.058740615844727\n",
      "__Loss is  25.05489158630371\n",
      "__Loss is  25.05228614807129\n",
      "Learning Rate is  2.89\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  26.032148361206055\n",
      "__Loss is  25.056568145751953\n",
      "__Loss is  25.06198501586914\n",
      "__Loss is  25.075593948364258\n",
      "__Loss is  25.145185470581055\n",
      "__Loss is  25.957435607910156\n",
      "__Loss is  26.101402282714844\n",
      "__Loss is  25.36969757080078\n",
      "__Loss is  25.440967559814453\n",
      "Learning Rate is  3.24\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.280715942382812\n",
      "__Loss is  25.077207565307617\n",
      "__Loss is  25.203054428100586\n",
      "__Loss is  27.91265106201172\n",
      "__Loss is  25.366605758666992\n",
      "__Loss is  25.485187530517578\n",
      "__Loss is  25.562532424926758\n",
      "__Loss is  27.14664649963379\n",
      "__Loss is  27.4853572845459\n",
      "Learning Rate is  3.61\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.345561981201172\n",
      "__Loss is  25.721195220947266\n",
      "__Loss is  25.704483032226562\n",
      "__Loss is  25.122310638427734\n",
      "__Loss is  25.871158599853516\n",
      "__Loss is  27.99376678466797\n",
      "__Loss is  25.548620223999023\n",
      "__Loss is  25.382768630981445\n",
      "__Loss is  25.31644630432129\n",
      "Learning Rate is  4.0\n",
      "__Loss is  28.216541290283203\n",
      "__Loss is  25.299928665161133\n",
      "__Loss is  25.45255470275879\n",
      "__Loss is  25.61223030090332\n",
      "__Loss is  25.403076171875\n",
      "__Loss is  25.048128128051758\n",
      "__Loss is  25.050865173339844\n",
      "__Loss is  25.063495635986328\n",
      "__Loss is  25.15510368347168\n",
      "__Loss is  28.09589958190918\n"
     ]
    }
   ],
   "source": [
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "\n",
    "    \n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.SGD(model2.parameters(),lr=l)\n",
    "    f_loss = MSELoss(reduction='mean')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 27.802043914794922  Accuracy: 0.54\n",
      "Learning Rate: 0.04 loss: 25.853771209716797  Accuracy: 0.54\n",
      "Learning Rate: 0.09 loss: 25.16435432434082  Accuracy: 0.47\n",
      "Learning Rate: 0.16 loss: 25.078956604003906  Accuracy: 0.4\n",
      "Learning Rate: 0.25 loss: 25.054494857788086  Accuracy: 0.49\n",
      "Learning Rate: 0.36 loss: 25.047836303710938  Accuracy: 0.5\n",
      "Learning Rate: 0.49 loss: 25.04714584350586  Accuracy: 0.5\n",
      "Learning Rate: 0.64 loss: 25.047090530395508  Accuracy: 0.51\n",
      "Learning Rate: 0.81 loss: 25.047080993652344  Accuracy: 0.51\n",
      "Learning Rate: 1.0 loss: 25.047080993652344  Accuracy: 0.51\n",
      "Learning Rate: 1.21 loss: 25.04707908630371  Accuracy: 0.51\n",
      "Learning Rate: 1.44 loss: 25.04707908630371  Accuracy: 0.51\n",
      "Learning Rate: 1.69 loss: 25.047134399414062  Accuracy: 0.5\n",
      "Learning Rate: 1.96 loss: 25.188018798828125  Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('SGD_3c.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/50 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/50\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"SGD_Accuracy_3c.txt\",lvA,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
