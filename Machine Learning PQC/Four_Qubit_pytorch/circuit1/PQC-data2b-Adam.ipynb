{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data2b.txt'\n",
    "data0Label = r'../../../dataset/data2blabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('Î¸'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.ry(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  56.6229248046875\n",
      "__Loss is  56.30297088623047\n",
      "__Loss is  55.992828369140625\n",
      "__Loss is  55.69266891479492\n",
      "__Loss is  55.40260696411133\n",
      "__Loss is  55.122745513916016\n",
      "__Loss is  54.85313415527344\n",
      "__Loss is  54.59383773803711\n",
      "__Loss is  54.34483337402344\n",
      "__Loss is  54.10609817504883\n",
      "Learning Rate is  0.04\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  55.40035629272461\n",
      "__Loss is  54.334617614746094\n",
      "__Loss is  53.42350769042969\n",
      "__Loss is  52.66044616699219\n",
      "__Loss is  52.0347900390625\n",
      "__Loss is  51.53270721435547\n",
      "__Loss is  51.138343811035156\n",
      "__Loss is  50.83489227294922\n",
      "__Loss is  50.605953216552734\n",
      "Learning Rate is  0.09\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  54.08503341674805\n",
      "__Loss is  52.31222915649414\n",
      "__Loss is  51.19462585449219\n",
      "__Loss is  50.56358337402344\n",
      "__Loss is  50.24502944946289\n",
      "__Loss is  50.101009368896484\n",
      "__Loss is  50.042606353759766\n",
      "__Loss is  50.0213737487793\n",
      "__Loss is  50.01476287841797\n",
      "Learning Rate is  0.16\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  52.624961853027344\n",
      "__Loss is  50.77188491821289\n",
      "__Loss is  50.166046142578125\n",
      "__Loss is  50.031856536865234\n",
      "__Loss is  50.013954162597656\n",
      "__Loss is  50.02216720581055\n",
      "__Loss is  50.061805725097656\n",
      "__Loss is  50.153072357177734\n",
      "__Loss is  50.29656982421875\n",
      "Learning Rate is  0.25\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  51.33475112915039\n",
      "__Loss is  50.12192916870117\n",
      "__Loss is  50.014503479003906\n",
      "__Loss is  50.04022979736328\n",
      "__Loss is  50.221527099609375\n",
      "__Loss is  50.6346321105957\n",
      "__Loss is  51.077606201171875\n",
      "__Loss is  51.22758865356445\n",
      "__Loss is  51.01996994018555\n",
      "Learning Rate is  0.36\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.46951675415039\n",
      "__Loss is  50.01483917236328\n",
      "__Loss is  50.106536865234375\n",
      "__Loss is  50.78142547607422\n",
      "__Loss is  51.7557373046875\n",
      "__Loss is  51.826751708984375\n",
      "__Loss is  51.10749816894531\n",
      "__Loss is  50.42729568481445\n",
      "__Loss is  50.11155319213867\n",
      "Learning Rate is  0.49\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.089969635009766\n",
      "__Loss is  50.07520294189453\n",
      "__Loss is  51.36247634887695\n",
      "__Loss is  52.91071319580078\n",
      "__Loss is  51.98630905151367\n",
      "__Loss is  50.61389923095703\n",
      "__Loss is  50.0921516418457\n",
      "__Loss is  50.02705764770508\n",
      "__Loss is  50.03794860839844\n",
      "Learning Rate is  0.64\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.01482391357422\n",
      "__Loss is  51.1824951171875\n",
      "__Loss is  54.58684158325195\n",
      "__Loss is  52.46470260620117\n",
      "__Loss is  50.3792724609375\n",
      "__Loss is  50.031009674072266\n",
      "__Loss is  50.04308319091797\n",
      "__Loss is  50.23188018798828\n",
      "__Loss is  50.83445358276367\n",
      "Learning Rate is  0.81\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.05369567871094\n",
      "__Loss is  56.74174118041992\n",
      "__Loss is  54.36890411376953\n",
      "__Loss is  50.44683837890625\n",
      "__Loss is  50.02628707885742\n",
      "__Loss is  50.134796142578125\n",
      "__Loss is  51.10263442993164\n",
      "__Loss is  52.216529846191406\n",
      "__Loss is  51.446693420410156\n",
      "Learning Rate is  1.0\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.61483383178711\n",
      "__Loss is  64.04779052734375\n",
      "__Loss is  53.39195251464844\n",
      "__Loss is  50.04138946533203\n",
      "__Loss is  50.19981384277344\n",
      "__Loss is  52.90474319458008\n",
      "__Loss is  54.516361236572266\n",
      "__Loss is  51.45496368408203\n",
      "__Loss is  50.811946868896484\n",
      "Learning Rate is  1.21\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  53.3197135925293\n",
      "__Loss is  59.0334358215332\n",
      "__Loss is  50.36696243286133\n",
      "__Loss is  50.06495666503906\n",
      "__Loss is  51.53505325317383\n",
      "__Loss is  53.51424026489258\n",
      "__Loss is  50.75466537475586\n",
      "__Loss is  52.20327377319336\n",
      "__Loss is  52.691673278808594\n",
      "Learning Rate is  1.44\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  60.588932037353516\n",
      "__Loss is  52.66033172607422\n",
      "__Loss is  50.03315734863281\n",
      "__Loss is  50.75436019897461\n",
      "__Loss is  53.872074127197266\n",
      "__Loss is  51.07145690917969\n",
      "__Loss is  51.5981330871582\n",
      "__Loss is  52.635398864746094\n",
      "__Loss is  50.665897369384766\n",
      "Learning Rate is  1.69\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  73.23120880126953\n",
      "__Loss is  52.58807373046875\n",
      "__Loss is  50.073577880859375\n",
      "__Loss is  57.01859664916992\n",
      "__Loss is  55.00080871582031\n",
      "__Loss is  51.12850570678711\n",
      "__Loss is  55.53459930419922\n",
      "__Loss is  52.24380111694336\n",
      "__Loss is  50.137020111083984\n",
      "Learning Rate is  1.96\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  87.12216186523438\n",
      "__Loss is  68.82427215576172\n",
      "__Loss is  50.094886779785156\n",
      "__Loss is  68.36406707763672\n",
      "__Loss is  57.922386169433594\n",
      "__Loss is  54.86587142944336\n",
      "__Loss is  56.17654800415039\n",
      "__Loss is  50.32522201538086\n",
      "__Loss is  56.55462646484375\n",
      "Learning Rate is  2.25\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  93.29877471923828\n",
      "__Loss is  50.045841217041016\n",
      "__Loss is  84.56791687011719\n",
      "__Loss is  54.93605041503906\n",
      "__Loss is  58.745323181152344\n",
      "__Loss is  65.4458236694336\n",
      "__Loss is  55.19756317138672\n",
      "__Loss is  51.84332275390625\n",
      "__Loss is  50.352928161621094\n",
      "Learning Rate is  2.56\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  85.1656723022461\n",
      "__Loss is  87.25447845458984\n",
      "__Loss is  83.41189575195312\n",
      "__Loss is  50.464324951171875\n",
      "__Loss is  66.1137466430664\n",
      "__Loss is  52.148502349853516\n",
      "__Loss is  50.16196060180664\n",
      "__Loss is  60.5136833190918\n",
      "__Loss is  61.56892013549805\n",
      "Learning Rate is  2.89\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  67.55056762695312\n",
      "__Loss is  83.44405364990234\n",
      "__Loss is  92.6446304321289\n",
      "__Loss is  79.63220977783203\n",
      "__Loss is  70.95793151855469\n",
      "__Loss is  51.26863098144531\n",
      "__Loss is  66.09070587158203\n",
      "__Loss is  80.25164794921875\n",
      "__Loss is  92.00908660888672\n",
      "Learning Rate is  3.24\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  53.886661529541016\n",
      "__Loss is  53.29676818847656\n",
      "__Loss is  53.568389892578125\n",
      "__Loss is  53.849586486816406\n",
      "__Loss is  53.842525482177734\n",
      "__Loss is  53.627906799316406\n",
      "__Loss is  53.41942596435547\n",
      "__Loss is  53.35787582397461\n",
      "__Loss is  53.43748092651367\n",
      "Learning Rate is  3.61\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.122337341308594\n",
      "__Loss is  61.03327560424805\n",
      "__Loss is  66.26629638671875\n",
      "__Loss is  61.32774353027344\n",
      "__Loss is  53.89825439453125\n",
      "__Loss is  51.38490676879883\n",
      "__Loss is  52.20896911621094\n",
      "__Loss is  55.73835372924805\n",
      "__Loss is  58.227699279785156\n",
      "Learning Rate is  4.0\n",
      "__Loss is  56.6229248046875\n",
      "__Loss is  50.10929489135742\n",
      "__Loss is  51.31999206542969\n",
      "__Loss is  65.63201904296875\n",
      "__Loss is  65.06183624267578\n",
      "__Loss is  53.85576629638672\n",
      "__Loss is  50.314170837402344\n",
      "__Loss is  50.554019927978516\n",
      "__Loss is  54.84543991088867\n",
      "__Loss is  58.877376556396484\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=l)\n",
    "    f_loss = MSELoss(reduction='sum')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 54.10609817504883  Accuracy: 0.63\n",
      "Learning Rate: 0.04 loss: 50.605953216552734  Accuracy: 0.63\n",
      "Learning Rate: 0.09 loss: 50.01476287841797  Accuracy: 0.45\n",
      "Learning Rate: 0.16 loss: 50.29656982421875  Accuracy: 0.63\n",
      "Learning Rate: 0.25 loss: 51.01996994018555  Accuracy: 0.63\n",
      "Learning Rate: 0.36 loss: 50.11155319213867  Accuracy: 0.53\n",
      "Learning Rate: 0.49 loss: 50.03794860839844  Accuracy: 0.49\n",
      "Learning Rate: 0.64 loss: 50.83445358276367  Accuracy: 0.63\n",
      "Learning Rate: 0.81 loss: 51.446693420410156  Accuracy: 0.49\n",
      "Learning Rate: 1.0 loss: 50.811946868896484  Accuracy: 0.37\n",
      "Learning Rate: 1.21 loss: 52.691673278808594  Accuracy: 0.47\n",
      "Learning Rate: 1.44 loss: 50.665897369384766  Accuracy: 0.49\n",
      "Learning Rate: 1.69 loss: 50.137020111083984  Accuracy: 0.49\n",
      "Learning Rate: 1.96 loss: 56.55462646484375  Accuracy: 0.63\n",
      "Learning Rate: 2.25 loss: 50.352928161621094  Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('Adam_2b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/100 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/100\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"Adam_Accuracy_2b.txt\",lvA,fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
