{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data3a.txt'\n",
    "data0Label = r'../../../dataset/data3alabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('Î¸'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.ry(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  56.7530632019043\n",
      "__Loss is  56.42607879638672\n",
      "__Loss is  56.10918426513672\n",
      "__Loss is  55.80253219604492\n",
      "__Loss is  55.506256103515625\n",
      "__Loss is  55.2204704284668\n",
      "__Loss is  54.945186614990234\n",
      "__Loss is  54.680477142333984\n",
      "__Loss is  54.42634582519531\n",
      "__Loss is  54.182743072509766\n",
      "Learning Rate is  0.04\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  55.50392532348633\n",
      "__Loss is  54.41586685180664\n",
      "__Loss is  53.48637008666992\n",
      "__Loss is  52.70846176147461\n",
      "__Loss is  52.07106399536133\n",
      "__Loss is  51.559871673583984\n",
      "__Loss is  51.15849685668945\n",
      "__Loss is  50.849727630615234\n",
      "__Loss is  50.61672592163086\n",
      "Learning Rate is  0.09\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  54.16111755371094\n",
      "__Loss is  52.35358428955078\n",
      "__Loss is  51.21562957763672\n",
      "__Loss is  50.57345199584961\n",
      "__Loss is  50.24876022338867\n",
      "__Loss is  50.10111618041992\n",
      "__Loss is  50.04032897949219\n",
      "__Loss is  50.01750564575195\n",
      "__Loss is  50.00993347167969\n",
      "Learning Rate is  0.16\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  52.67204284667969\n",
      "__Loss is  50.78536605834961\n",
      "__Loss is  50.16790008544922\n",
      "__Loss is  50.028892517089844\n",
      "__Loss is  50.0089225769043\n",
      "__Loss is  50.01811218261719\n",
      "__Loss is  50.06254196166992\n",
      "__Loss is  50.16215515136719\n",
      "__Loss is  50.315216064453125\n",
      "Learning Rate is  0.25\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  51.35779571533203\n",
      "__Loss is  50.12260437011719\n",
      "__Loss is  50.00962448120117\n",
      "__Loss is  50.038516998291016\n",
      "__Loss is  50.236228942871094\n",
      "__Loss is  50.672420501708984\n",
      "__Loss is  51.12464904785156\n",
      "__Loss is  51.26215362548828\n",
      "__Loss is  51.03433609008789\n",
      "Learning Rate is  0.36\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.477108001708984\n",
      "__Loss is  50.010040283203125\n",
      "__Loss is  50.11201477050781\n",
      "__Loss is  50.83020782470703\n",
      "__Loss is  51.825721740722656\n",
      "__Loss is  51.86498260498047\n",
      "__Loss is  51.1133918762207\n",
      "__Loss is  50.423099517822266\n",
      "__Loss is  50.1055793762207\n",
      "Learning Rate is  0.49\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.08944320678711\n",
      "__Loss is  50.077850341796875\n",
      "__Loss is  51.444732666015625\n",
      "__Loss is  52.999237060546875\n",
      "__Loss is  52.00624465942383\n",
      "__Loss is  50.609527587890625\n",
      "__Loss is  50.085575103759766\n",
      "__Loss is  50.01335144042969\n",
      "__Loss is  50.02799987792969\n",
      "Learning Rate is  0.64\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.010032653808594\n",
      "__Loss is  51.26412582397461\n",
      "__Loss is  54.7154426574707\n",
      "__Loss is  52.480491638183594\n",
      "__Loss is  50.374183654785156\n",
      "__Loss is  50.01884078979492\n",
      "__Loss is  50.03522872924805\n",
      "__Loss is  50.29693603515625\n",
      "__Loss is  50.99359893798828\n",
      "Learning Rate is  0.81\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.053768157958984\n",
      "__Loss is  57.03706741333008\n",
      "__Loss is  54.440582275390625\n",
      "__Loss is  50.44195556640625\n",
      "__Loss is  50.013065338134766\n",
      "__Loss is  50.16985321044922\n",
      "__Loss is  51.31671142578125\n",
      "__Loss is  52.46957778930664\n",
      "__Loss is  51.60408020019531\n",
      "Learning Rate is  1.0\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.65964126586914\n",
      "__Loss is  64.34233093261719\n",
      "__Loss is  53.38089370727539\n",
      "__Loss is  50.03030014038086\n",
      "__Loss is  50.256813049316406\n",
      "__Loss is  53.33698654174805\n",
      "__Loss is  54.98978805541992\n",
      "__Loss is  51.77498245239258\n",
      "__Loss is  50.2867431640625\n",
      "Learning Rate is  1.21\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  53.510414123535156\n",
      "__Loss is  58.976043701171875\n",
      "__Loss is  50.35221862792969\n",
      "__Loss is  50.04874038696289\n",
      "__Loss is  51.93900680541992\n",
      "__Loss is  53.939537048339844\n",
      "__Loss is  50.87469482421875\n",
      "__Loss is  51.08371353149414\n",
      "__Loss is  52.50224304199219\n",
      "Learning Rate is  1.44\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  61.08307647705078\n",
      "__Loss is  52.64813232421875\n",
      "__Loss is  50.0150260925293\n",
      "__Loss is  50.96808624267578\n",
      "__Loss is  54.32979965209961\n",
      "__Loss is  51.314579010009766\n",
      "__Loss is  50.590850830078125\n",
      "__Loss is  51.83612060546875\n",
      "__Loss is  50.83975601196289\n",
      "Learning Rate is  1.69\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  74.15380096435547\n",
      "__Loss is  52.659725189208984\n",
      "__Loss is  50.081607818603516\n",
      "__Loss is  57.89053726196289\n",
      "__Loss is  55.918025970458984\n",
      "__Loss is  50.323974609375\n",
      "__Loss is  52.866275787353516\n",
      "__Loss is  51.308433532714844\n",
      "__Loss is  50.755836486816406\n",
      "Learning Rate is  1.96\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  88.41006469726562\n",
      "__Loss is  69.85192108154297\n",
      "__Loss is  50.04899978637695\n",
      "__Loss is  71.16763305664062\n",
      "__Loss is  63.29016876220703\n",
      "__Loss is  53.02115249633789\n",
      "__Loss is  64.8731460571289\n",
      "__Loss is  56.35433578491211\n",
      "__Loss is  50.439247131347656\n",
      "Learning Rate is  2.25\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  94.62290954589844\n",
      "__Loss is  50.02312469482422\n",
      "__Loss is  88.7669677734375\n",
      "__Loss is  62.1947135925293\n",
      "__Loss is  55.3841552734375\n",
      "__Loss is  69.01577758789062\n",
      "__Loss is  53.13636016845703\n",
      "__Loss is  51.05759811401367\n",
      "__Loss is  53.01381301879883\n",
      "Learning Rate is  2.56\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  86.10667419433594\n",
      "__Loss is  88.45738220214844\n",
      "__Loss is  84.56182098388672\n",
      "__Loss is  50.0960578918457\n",
      "__Loss is  73.58111572265625\n",
      "__Loss is  57.23229217529297\n",
      "__Loss is  50.55047607421875\n",
      "__Loss is  59.13808059692383\n",
      "__Loss is  60.21522521972656\n",
      "Learning Rate is  2.89\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  67.95003509521484\n",
      "__Loss is  84.37985229492188\n",
      "__Loss is  93.93075561523438\n",
      "__Loss is  80.98388671875\n",
      "__Loss is  72.47836303710938\n",
      "__Loss is  50.441097259521484\n",
      "__Loss is  72.83048248291016\n",
      "__Loss is  71.79508209228516\n",
      "__Loss is  83.52528381347656\n",
      "Learning Rate is  3.24\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  53.958702087402344\n",
      "__Loss is  53.369319915771484\n",
      "__Loss is  53.650550842285156\n",
      "__Loss is  53.93341064453125\n",
      "__Loss is  53.920387268066406\n",
      "__Loss is  53.69916915893555\n",
      "__Loss is  53.48869705200195\n",
      "__Loss is  53.429527282714844\n",
      "__Loss is  53.51148223876953\n",
      "Learning Rate is  3.61\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.12275314331055\n",
      "__Loss is  61.23588943481445\n",
      "__Loss is  66.5840072631836\n",
      "__Loss is  61.532379150390625\n",
      "__Loss is  53.95710372924805\n",
      "__Loss is  51.420902252197266\n",
      "__Loss is  52.283443450927734\n",
      "__Loss is  55.89689254760742\n",
      "__Loss is  58.38882827758789\n",
      "Learning Rate is  4.0\n",
      "__Loss is  56.7530632019043\n",
      "__Loss is  50.11526870727539\n",
      "__Loss is  51.47240447998047\n",
      "__Loss is  65.2515869140625\n",
      "__Loss is  63.79148864746094\n",
      "__Loss is  52.988433837890625\n",
      "__Loss is  50.31248474121094\n",
      "__Loss is  50.787109375\n",
      "__Loss is  55.63431930541992\n",
      "__Loss is  58.35549545288086\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.Adam(model2.parameters(), lr=l)\n",
    "    f_loss = MSELoss(reduction='sum')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 54.182743072509766  Accuracy: 0.54\n",
      "Learning Rate: 0.04 loss: 50.61672592163086  Accuracy: 0.54\n",
      "Learning Rate: 0.09 loss: 50.00993347167969  Accuracy: 0.46\n",
      "Learning Rate: 0.16 loss: 50.315216064453125  Accuracy: 0.54\n",
      "Learning Rate: 0.25 loss: 51.03433609008789  Accuracy: 0.54\n",
      "Learning Rate: 0.36 loss: 50.1055793762207  Accuracy: 0.59\n",
      "Learning Rate: 0.49 loss: 50.02799987792969  Accuracy: 0.66\n",
      "Learning Rate: 0.64 loss: 50.99359893798828  Accuracy: 0.54\n",
      "Learning Rate: 0.81 loss: 51.60408020019531  Accuracy: 0.67\n",
      "Learning Rate: 1.0 loss: 50.2867431640625  Accuracy: 0.46\n",
      "Learning Rate: 1.21 loss: 52.50224304199219  Accuracy: 0.46\n",
      "Learning Rate: 1.44 loss: 50.83975601196289  Accuracy: 0.46\n",
      "Learning Rate: 1.69 loss: 50.755836486816406  Accuracy: 0.54\n",
      "Learning Rate: 1.96 loss: 50.439247131347656  Accuracy: 0.54\n",
      "Learning Rate: 2.25 loss: 53.01381301879883  Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('Adam_3a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/100 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/100\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"Adam_Accuracy_3a.txt\",lvA,fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
