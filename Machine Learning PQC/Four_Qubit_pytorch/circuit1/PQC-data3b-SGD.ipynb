{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qiskit-terra': '0.17.1', 'qiskit-aer': '0.8.1', 'qiskit-ignis': '0.6.0', 'qiskit-ibmq-provider': '0.12.2', 'qiskit-aqua': '0.9.1', 'qiskit': '0.25.1', 'qiskit-nature': None, 'qiskit-finance': None, 'qiskit-optimization': None, 'qiskit-machine-learning': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qiskit\n",
    "qiskit.__qiskit_version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS, SGD,Adam \n",
    "\n",
    "from qiskit  import Aer, QuantumCircuit\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.opflow import AerPauliExpectation\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_machine_learning.neural_networks import CircuitQNN, TwoLayerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "qi = QuantumInstance(Aer.get_backend('statevector_simulator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.nn import (Module, Conv2d, Linear, Dropout2d, NLLLoss,\n",
    "                     MaxPool2d, Flatten, Sequential, ReLU)\n",
    "\n",
    "data0Path = r'../../../dataset/data3b.txt'\n",
    "data0Label = r'../../../dataset/data3blabel.txt'\n",
    "\n",
    "dataCoords = np.loadtxt(data0Path)\n",
    "dataLabels = np.loadtxt(data0Label)\n",
    "\n",
    "# Make a data structure which is easier to work with\n",
    "# for shuffling. \n",
    "# Also, notice we change the data labels from {0, 1} to {-1, +1}\n",
    "data1 = list(zip(dataCoords, 2*dataLabels-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(x):\n",
    "    return ('0'*(4-len('{:b}'.format(x) ))+'{:b}'.format(x))\n",
    "def firsttwo(x):\n",
    "    return x[:2]\n",
    "parity = lambda x: firsttwo(binary(x)).count('1') % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 206.997x264.88 with 1 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "#data_ixs = np.random.choice(len(shuffled_data), size=len(shuffled_data))\n",
    "data_ixs = np.random.choice(len(data1), size=100)\n",
    "\n",
    "X= [np.array(list(flatten([data1[j][0],data1[j][0]]))) for j in data_ixs]\n",
    "y = [data1[j][1] for j in data_ixs]\n",
    "y01 =  [ (x + 1)/2 for x in y]\n",
    "X_ = Tensor(X)\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "\n",
    "num_inputs=4;\n",
    "\n",
    "feature_map = QuantumCircuit(4, name='Embed')\n",
    "feature_map.rx(Parameter('x[0]'),0)\n",
    "feature_map.rx(Parameter('x[1]'),1)\n",
    "feature_map.rx(Parameter('x[2]'),2)\n",
    "feature_map.rx(Parameter('x[3]'),3)\n",
    "feature_map.ry(pi/4,0)\n",
    "feature_map.ry(pi/4,1)\n",
    "feature_map.ry(pi/4,2)\n",
    "feature_map.ry(pi/4,3)\n",
    "feature_map.rz(pi/4,0)\n",
    "feature_map.rz(pi/4,1)\n",
    "feature_map.rz(pi/4,2)\n",
    "feature_map.rz(pi/4,3)\n",
    "\n",
    "\n",
    "param_y=[];\n",
    "for i in range(12):\n",
    "    param_y.append((Parameter('Î¸'+str(i))))\n",
    "ansatz = QuantumCircuit(4, name='PQC')\n",
    "for i in range(4):\n",
    "    ansatz.ry(param_y[i],i)\n",
    "for i in range(4):\n",
    "    ansatz.rz(param_y[i+4],i)\n",
    "\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.append(feature_map, range(num_inputs))\n",
    "qc.append(ansatz, range(num_inputs))\n",
    "\n",
    "ansatz.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate is  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saesun Kim\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Loss is  28.600173950195312\n",
      "__Loss is  27.411035537719727\n",
      "__Loss is  26.68559455871582\n",
      "__Loss is  26.230384826660156\n",
      "__Loss is  25.932811737060547\n",
      "__Loss is  25.730098724365234\n",
      "__Loss is  25.58674430847168\n",
      "__Loss is  25.48201560974121\n",
      "__Loss is  25.40333366394043\n",
      "__Loss is  25.3427734375\n",
      "Learning Rate is  0.04\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.45770835876465\n",
      "__Loss is  25.21772003173828\n",
      "__Loss is  25.133602142333984\n",
      "__Loss is  25.09248161315918\n",
      "__Loss is  25.06877899169922\n",
      "__Loss is  25.053695678710938\n",
      "__Loss is  25.043428421020508\n",
      "__Loss is  25.03609275817871\n",
      "__Loss is  25.030656814575195\n",
      "Learning Rate is  0.09\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.015945434570312\n",
      "__Loss is  25.013591766357422\n",
      "__Loss is  25.011993408203125\n",
      "__Loss is  25.01087188720703\n",
      "__Loss is  25.01008415222168\n",
      "__Loss is  25.009517669677734\n",
      "__Loss is  25.009103775024414\n",
      "__Loss is  25.00881004333496\n",
      "__Loss is  25.008586883544922\n",
      "Learning Rate is  0.16\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  27.95116424560547\n",
      "__Loss is  26.58449935913086\n",
      "__Loss is  25.156789779663086\n",
      "__Loss is  25.040937423706055\n",
      "__Loss is  25.031999588012695\n",
      "__Loss is  25.025413513183594\n",
      "__Loss is  25.02060890197754\n",
      "__Loss is  25.017166137695312\n",
      "__Loss is  25.014738082885742\n",
      "Learning Rate is  0.25\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  44.349632263183594\n",
      "__Loss is  26.204769134521484\n",
      "__Loss is  26.237079620361328\n",
      "__Loss is  38.88764953613281\n",
      "__Loss is  28.491680145263672\n",
      "__Loss is  42.704566955566406\n",
      "__Loss is  30.463802337646484\n",
      "__Loss is  25.04252052307129\n",
      "__Loss is  25.029932022094727\n",
      "Learning Rate is  0.36\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  32.629764556884766\n",
      "__Loss is  25.15275764465332\n",
      "__Loss is  25.139162063598633\n",
      "__Loss is  25.097978591918945\n",
      "__Loss is  25.00868797302246\n",
      "__Loss is  25.008487701416016\n",
      "__Loss is  25.008392333984375\n",
      "__Loss is  25.00831413269043\n",
      "__Loss is  25.008243560791016\n",
      "Learning Rate is  0.49\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.093801498413086\n",
      "__Loss is  25.019023895263672\n",
      "__Loss is  25.00950050354004\n",
      "__Loss is  25.00912094116211\n",
      "__Loss is  25.00890350341797\n",
      "__Loss is  25.008745193481445\n",
      "__Loss is  25.00861167907715\n",
      "__Loss is  25.00849723815918\n",
      "__Loss is  25.00838851928711\n",
      "Learning Rate is  0.64\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  44.91114044189453\n",
      "__Loss is  30.757213592529297\n",
      "__Loss is  25.481048583984375\n",
      "__Loss is  28.14378547668457\n",
      "__Loss is  30.080280303955078\n",
      "__Loss is  26.413862228393555\n",
      "__Loss is  33.86042022705078\n",
      "__Loss is  26.339401245117188\n",
      "__Loss is  27.338329315185547\n",
      "Learning Rate is  0.81\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.395709991455078\n",
      "__Loss is  36.1254768371582\n",
      "__Loss is  31.352575302124023\n",
      "__Loss is  25.97324562072754\n",
      "__Loss is  40.363075256347656\n",
      "__Loss is  37.74119567871094\n",
      "__Loss is  38.05899429321289\n",
      "__Loss is  37.71442794799805\n",
      "__Loss is  30.14754867553711\n",
      "Learning Rate is  1.0\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  41.20806884765625\n",
      "__Loss is  29.428186416625977\n",
      "__Loss is  41.07390213012695\n",
      "__Loss is  33.830326080322266\n",
      "__Loss is  27.25303077697754\n",
      "__Loss is  25.750534057617188\n",
      "__Loss is  41.93888854980469\n",
      "__Loss is  29.227783203125\n",
      "__Loss is  26.29821014404297\n",
      "Learning Rate is  1.21\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.097484588623047\n",
      "__Loss is  25.986618041992188\n",
      "__Loss is  26.44647216796875\n",
      "__Loss is  35.519405364990234\n",
      "__Loss is  29.40500831604004\n",
      "__Loss is  25.416461944580078\n",
      "__Loss is  33.70716857910156\n",
      "__Loss is  41.86414337158203\n",
      "__Loss is  33.2520751953125\n",
      "Learning Rate is  1.44\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  45.09321212768555\n",
      "__Loss is  39.21493148803711\n",
      "__Loss is  37.09868240356445\n",
      "__Loss is  25.698381423950195\n",
      "__Loss is  27.313753128051758\n",
      "__Loss is  38.028141021728516\n",
      "__Loss is  26.491912841796875\n",
      "__Loss is  25.30622100830078\n",
      "__Loss is  38.56826400756836\n",
      "Learning Rate is  1.69\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  27.017385482788086\n",
      "__Loss is  26.047067642211914\n",
      "__Loss is  43.98986053466797\n",
      "__Loss is  30.188976287841797\n",
      "__Loss is  27.58966827392578\n",
      "__Loss is  25.505464553833008\n",
      "__Loss is  29.883480072021484\n",
      "__Loss is  39.02606964111328\n",
      "__Loss is  25.026796340942383\n",
      "Learning Rate is  1.96\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  25.407001495361328\n",
      "__Loss is  43.328712463378906\n",
      "__Loss is  26.65072250366211\n",
      "__Loss is  36.55073165893555\n",
      "__Loss is  35.519432067871094\n",
      "__Loss is  39.504390716552734\n",
      "__Loss is  45.00832748413086\n",
      "__Loss is  25.690183639526367\n",
      "__Loss is  26.316450119018555\n",
      "Learning Rate is  2.25\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  38.38932800292969\n",
      "__Loss is  37.60109329223633\n",
      "__Loss is  25.825708389282227\n",
      "__Loss is  25.075037002563477\n",
      "__Loss is  28.896944046020508\n",
      "__Loss is  25.73476791381836\n",
      "__Loss is  34.75450134277344\n",
      "__Loss is  25.133203506469727\n",
      "__Loss is  36.9927864074707\n",
      "Learning Rate is  2.56\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  43.53363800048828\n",
      "__Loss is  34.559303283691406\n",
      "__Loss is  25.102853775024414\n",
      "__Loss is  41.694400787353516\n",
      "__Loss is  34.45262908935547\n",
      "__Loss is  26.845842361450195\n",
      "__Loss is  26.85917854309082\n",
      "__Loss is  28.8957462310791\n",
      "__Loss is  26.0288143157959\n",
      "Learning Rate is  2.89\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  33.68859100341797\n",
      "__Loss is  26.30274772644043\n",
      "__Loss is  29.739831924438477\n",
      "__Loss is  31.273839950561523\n",
      "__Loss is  25.98508071899414\n",
      "__Loss is  30.06201934814453\n",
      "__Loss is  32.99014663696289\n",
      "__Loss is  27.038267135620117\n",
      "__Loss is  25.41315460205078\n",
      "Learning Rate is  3.24\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  27.890304565429688\n",
      "__Loss is  30.24675750732422\n",
      "__Loss is  25.049781799316406\n",
      "__Loss is  32.721317291259766\n",
      "__Loss is  25.033876419067383\n",
      "__Loss is  27.425010681152344\n",
      "__Loss is  26.52120590209961\n",
      "__Loss is  30.987224578857422\n",
      "__Loss is  25.19681739807129\n",
      "Learning Rate is  3.61\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  26.386140823364258\n",
      "__Loss is  35.34718704223633\n",
      "__Loss is  25.48596954345703\n",
      "__Loss is  28.758596420288086\n",
      "__Loss is  28.042312622070312\n",
      "__Loss is  35.52333450317383\n",
      "__Loss is  42.78638458251953\n",
      "__Loss is  25.8519229888916\n",
      "__Loss is  27.81296157836914\n",
      "Learning Rate is  4.0\n",
      "__Loss is  28.600173950195312\n",
      "__Loss is  26.70099639892578\n",
      "__Loss is  25.78264617919922\n",
      "__Loss is  25.585294723510742\n",
      "__Loss is  27.50071907043457\n",
      "__Loss is  37.686153411865234\n",
      "__Loss is  29.755142211914062\n",
      "__Loss is  25.93288803100586\n",
      "__Loss is  27.64893341064453\n",
      "__Loss is  25.43440818786621\n"
     ]
    }
   ],
   "source": [
    "learningR=[round(((i+1)/10)**2,2) for i in range(20)]\n",
    "\n",
    "    \n",
    "output_shape=2;\n",
    "epochs = 10     # set number of epochs\n",
    "\n",
    "model=[];\n",
    "lossLR=[];\n",
    "\n",
    "\n",
    "\n",
    "for l in learningR:\n",
    "    np.random.seed(2)  \n",
    "    qnn2 = CircuitQNN(qc, input_params=feature_map.parameters, weight_params=ansatz.parameters, \n",
    "                      interpret=parity, output_shape=output_shape, quantum_instance=qi)\n",
    "    initial_weights = 0.1*(2*np.random.rand(qnn2.num_weights) - 1)\n",
    "    # set up PyTorch module\n",
    "    model2 = TorchConnector(qnn2, initial_weights)\n",
    "    # define optimizer and loss function\n",
    "    optimizer = optim.SGD(model2.parameters(),lr=l)\n",
    "    f_loss = MSELoss(reduction='mean')\n",
    "\n",
    "    # start training\n",
    "    model2.train()   # set model to training mode\n",
    "    \n",
    "    print(\"Learning Rate is \", l)\n",
    "    # define objective function\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()        # initialize gradient\n",
    "        loss = 0.0                                             # initialize loss    \n",
    "        for x, y_target in zip(X, y01):                        # evaluate batch loss\n",
    "            output = model2(Tensor(x)).reshape(1, 2)           # forward pass\n",
    "            targets=Tensor([y_target]).long()\n",
    "            targets = targets.to(torch.float32)\n",
    "            #targets = targets.unsqueeze(1)\n",
    "            loss += f_loss(output, targets) \n",
    "        loss.backward()                              # backward pass\n",
    "        print(\"__Loss is \",loss.item())                           # print loss\n",
    "\n",
    "        # run optimizer\n",
    "        optimizer.step() \n",
    "    model.append(model2)\n",
    "    lossLR.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01 loss: 25.3427734375  Accuracy: 0.54\n",
      "Learning Rate: 0.04 loss: 25.030656814575195  Accuracy: 0.62\n",
      "Learning Rate: 0.09 loss: 25.008586883544922  Accuracy: 0.8\n",
      "Learning Rate: 0.16 loss: 25.014738082885742  Accuracy: 0.66\n",
      "Learning Rate: 0.25 loss: 25.029932022094727  Accuracy: 0.57\n",
      "Learning Rate: 0.36 loss: 25.008243560791016  Accuracy: 0.79\n",
      "Learning Rate: 0.49 loss: 25.00838851928711  Accuracy: 0.78\n",
      "Learning Rate: 0.64 loss: 27.338329315185547  Accuracy: 0.52\n",
      "Learning Rate: 0.81 loss: 30.14754867553711  Accuracy: 0.6\n",
      "Learning Rate: 1.0 loss: 26.29821014404297  Accuracy: 0.45\n",
      "Learning Rate: 1.21 loss: 33.2520751953125  Accuracy: 0.46\n",
      "Learning Rate: 1.44 loss: 38.56826400756836  Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "ln=len(learningR)\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "accLR=[];\n",
    "for lr in range(ln):\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+1)\n",
    "    y_predict = []\n",
    "    for x in X:\n",
    "        output = model[lr](Tensor(x))\n",
    "        y_predict += [np.argmax(output.detach().numpy())]\n",
    "    acc=sum(y_predict == np.array(y01))/len(np.array(y01))\n",
    "    print('Learning Rate:', learningR[lr],'loss:',lossLR[lr],' Accuracy:', acc)\n",
    "    accLR.append(acc)\n",
    "    # plot results\n",
    "    # red == wrongly classified\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "        if y_target != y_:\n",
    "            ax.scatter(x[0], x[1], s=200, facecolors='none', edgecolors='r', linewidths=2)\n",
    "\n",
    "\n",
    "    ax = fig.add_subplot(ln, 2, 2*lr+2)\n",
    "    for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "        if y_target == 1:\n",
    "            ax.plot(x[0], x[1], 'bo')\n",
    "        else:\n",
    "            ax.plot(x[0], x[1], 'go')\n",
    "\n",
    "    X1 = np.linspace(0, 1, num=10)\n",
    "    Z1 = np.zeros((len(X1), len(X1)))\n",
    "\n",
    "    # Contour map\n",
    "    for j in range(len(X1)):\n",
    "        for k in range(len(X1)):\n",
    "            # Fill Z with the labels (numerical values)\n",
    "            # the inner loop goes over the columns of Z,\n",
    "            # which corresponds to sweeping x-values\n",
    "            # Therefore, the role of j,k is flipped in the signature\n",
    "            Z1[j, k] = np.argmax(model[lr](Tensor([X1[k],X1[j],X1[k],X1[j]])).detach().numpy())\n",
    "\n",
    "    ax.contourf(X1, X1, Z1, cmap='bwr', levels=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=fig\n",
    "f1.set_size_inches(10, 80)\n",
    "f1.savefig('SGD_3b.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossLR50=[x/50 for x in lossLR]\n",
    "plt.plot(learningR,lossLR50, label = \"Loss/50\")\n",
    "plt.plot(learningR,accLR, label = \"Accuracy\")\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "lvA=[[learningR[x],accLR[x]] for x in range(len(accLR))]\n",
    "np.savetxt(\"SGD_Accuracy_3b.txt\",lvA,fmt='%.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
